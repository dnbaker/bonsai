#pragma once
#include <cstdint>
#include <cstdlib>
#include <vector>
#include <type_traits>
#include <string>
#include "libpopcnt.h"
#include "hll/hll.h"


#ifndef DO_DUFF
#define DO_DUFF(len, ITER) \
    do { \
        if(len) {\
            std::uint64_t loop = (len + 7) >> 3;\
            switch(len & 7) {\
                case 0: do {\
                    ITER; [[fallthrough]];\
                    case 7: ITER; [[fallthrough]]; case 6: ITER; [[fallthrough]]; case 5: ITER; [[fallthrough]];\
                    case 4: ITER; [[fallthrough]]; case 3: ITER; [[fallthrough]]; case 2: ITER; [[fallthrough]]; case 1: ITER;\
                } while (--loop);\
            }\
        }\
    } while(0)
#endif

namespace pop {
using bitvec_t = std::vector<uint64_t>;

template<typename T>
inline unsigned popcount(T val)    noexcept {return __builtin_popcount(val);}
template<>
inline unsigned popcount(char val) noexcept {
/*
    LUT generated by python:
print("    static const uint8_t lut []{\n        %s\n    };" % ", ".join(map(str, (bin(i).count("1") for i in range(256)))))
*/
    static const uint8_t lut [] {
        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 4, 5, 5, 6, 5, 6, 6, 7, 5, 6, 6, 7, 6, 7, 7, 8
    };
    return static_cast<unsigned>(lut[(uint8_t)val]);
}

template<>
inline unsigned popcount(unsigned long long val) noexcept {
#ifndef NO_USE_CQF_ASM
// From cqf https://github.com/splatlab/cqf/
    asm("popcnt %[val], %[val]"
            : [val] "+r" (val)
            :
            : "cc");
    return val;
#else
    // According to GodBolt, gcc7.3 fails to inline this function call even at -Ofast.
    //
    //
    return __builtin_popcountll(val);
#endif
}

template<>
inline unsigned popcount(unsigned long val) noexcept {
    return popcount(static_cast<unsigned long long>(val));
}

inline unsigned vec_popcnt(const char *p, const size_t l) {
    return ::popcnt((void *)p, l);
}

inline unsigned vec_popcnt(uint64_t *p, size_t l) {
    if(__builtin_expect(l == 0, 0)) return 0;
    uint64_t ret(popcount(*p));
    while(--l) ret += popcount(*++p);
    return ret;
}

inline unsigned vec_popcnt(const std::string &vec) {
    return vec_popcnt(vec.data(), vec.size());
}

template<typename T>
inline auto vec_popcnt(const T &container) {
    auto i(container.cbegin());
    auto ret(popcount(*i));
    while(++i != container.cend()) ret += popcount(*i);
    return ret;
}

template<typename T, typename std::enable_if_t<std::is_arithmetic_v<T>>>
inline unsigned bitdiff(T a, T b) {
    // TODO: Modify to use SSE intrinsics to speed up calculation.
    // See https://github.com/WojciechMula/sse-popcount for examples/code.
    // Consider adding #ifndef wrappings based on architecture.
    return popcount(a ^ b);
}

namespace detail {

#if HAS_AVX_512
#define popcnt_fn(x) popcnt512(x.simd_)
#elif __AVX2__
#define popcnt_fn(x) popcnt256(x.simd_)
#elif __SSE2__
#define popcnt_fn(x) (popcount(x.arr_[0]) + popcount(x.arr_[1]))
#else
#error("Need SSE2")
#endif

inline unsigned unrolled_bitdiff(const uint64_t *a, const uint64_t *b, size_t nbytes);
inline unsigned byte_bitdiff(const uint8_t *a, const uint8_t *b, size_t nelem) {
    using SIMDHolder = vec::SIMDTypes<u64>::VType;
    size_t nblocks(nelem / (sizeof(SIMDHolder)));
    const SIMDHolder *pa((const SIMDHolder *)a), *pb((const SIMDHolder *)b);
    SIMDHolder tmp;
#if HAS_AVX_512 || __AVX2__
    SIMDHolder
#else
    unsigned
#endif
        sum;
    tmp = (pa++)->simd_ ^ (pb++)->simd_; // I'm being lazy here and assuming it's aligned, but I have that guarantee from the aligned vectors.
    sum = popcnt_fn(tmp);
    while(--nblocks) { // Prefix decrement to account for the fact that I used one block in initialization.
        tmp.simd_ = (pa++)->simd_ ^ (pb++)->simd_;
#if HAS_AVX_512 || __AVX2__
        sum.simd_ += popcnt_fn(tmp);
#else
        sum += popcnt_fn(tmp);
#endif
    }
    unsigned ret = unrolled_bitdiff((const uint64_t *)pa, (const uint64_t *)pb, nelem & ((nelem / (sizeof(SIMDHolder) / sizeof(uint8_t))) - 1));
#if HAS_AVX_512 || __AVX2__
    sum.for_each([&](const std::uint64_t &x) {ret += x;});
#else
    ret += sum;
#endif
    return ret;
}

inline unsigned unrolled_bitdiff(const uint64_t *a, const uint64_t *b, size_t nbytes) {
    unsigned ret(popcount(*a++ ^ *b++));
    nbytes -= 8;
    const size_t len(nbytes >> 3); // Num 64-bit integers.
#define ITER ret += popcount(*a++ ^ *b++)
    DO_DUFF(len, ITER);
#undef ITER
    if(__builtin_expect(nbytes &= 0x7u, 0)) {
        // I still haven't finished testing if this is correct or makes a big/little-endian mistake.
        // But, at worst, it's a matter of switching the right-shift for a left-shift.
        // ret += popcount((*a ^ *b) & ((0xFFFFFFFFFFFFFFFF >> ((8u - nbytes) << 3))));
        // The following has one less instruction in the assembly by using
        // "lea ecx, [64+rdx*8]", which puts the multiplication into the lea.
        ret += popcount((*a ^ *b) & ((0xFFFFFFFFFFFFFFFF >> ((8u - nbytes) * 8))));
    }
    return ret;
}
} // namespace detail

template<typename T>
inline auto vec_bitdiff(const T &a, const T &b) {
#if defined(USE_UNROLLED_BITDIFF)
    return detail::unrolled_bitdiff((const uint64_t *)&a[0], (const uint64_t *)&b[0], a.size() * sizeof(a[0]));
#else
    return detail::byte_bitdiff(&a[0], &b[0], a.size() * sizeof(a[0]));
#endif
}

} // namespace popcnt
