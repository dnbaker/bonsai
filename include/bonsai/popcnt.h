#pragma once
#include <cstdint>
#include <cstdlib>
#include <vector>
#include <type_traits>
#include <string>
#include "libpopcnt/libpopcnt.h"
#define NO_BLAZE
#define NO_SLEEF
#include "vec/vec.h"
#undef NO_BLAZE
#undef NO_SLEEF
#include "sketch/hll.h"


#ifndef DO_DUFF
#define DO_DUFF(len, ITER) \
    do { \
        if(len) {\
            std::uint64_t loop = (len + 7) >> 3;\
            switch(len & 7) {\
                case 0: do {\
                    ITER; [[fallthrough]];\
                    case 7: ITER; [[fallthrough]]; case 6: ITER; [[fallthrough]]; case 5: ITER; [[fallthrough]];\
                    case 4: ITER; [[fallthrough]]; case 3: ITER; [[fallthrough]]; case 2: ITER; [[fallthrough]]; case 1: ITER;\
                } while (--loop);\
            }\
        }\
    } while(0)
#endif

namespace pop {
using bitvec_t = std::vector<uint64_t>;

template<typename T>
inline constexpr unsigned popcount(T val) {return __builtin_popcount(val);}
template<>
inline constexpr unsigned popcount(char val) {
/*
    LUT generated by python:
print("    static const uint8_t lut []{\n        %s\n    };" % ", ".join(map(str, (bin(i).count("1") for i in range(256)))))
*/
    constexpr uint8_t lut [] {
        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7, 4, 5, 5, 6, 5, 6, 6, 7, 5, 6, 6, 7, 6, 7, 7, 8
    };
    return static_cast<unsigned>(lut[(uint8_t)val]);
}

template<>
inline unsigned popcount(unsigned long long val) {
    return __builtin_popcountll(val);
}

template<>
inline unsigned popcount(unsigned long val) {
    return popcount(static_cast<unsigned long long>(val));
}

inline unsigned vec_popcnt(const char *p, const size_t l) {
    return ::popcnt((void *)p, l);
}

inline unsigned vec_popcnt(uint64_t *p, size_t l) {
    if(__builtin_expect(l == 0, 0)) return 0;
    uint64_t ret(popcount(*p));
    while(--l) ret += popcount(*++p);
    return ret;
}

inline unsigned vec_popcnt(const std::string &vec) {
    return vec_popcnt(vec.data(), vec.size());
}

template<typename T>
inline auto vec_popcnt(const T &container) {
    auto i(container.cbegin());
    auto ret(popcount(*i));
    while(++i != container.cend()) ret += popcount(*i);
    return ret;
}

template<typename T, typename std::enable_if<std::is_arithmetic<T>::value>::type>
inline unsigned bitdiff(T a, T b) {
    // TODO: Modify to use SSE intrinsics to speed up calculation.
    // See https://github.com/WojciechMula/sse-popcount for examples/code.
    // Consider adding #ifndef wrappings based on architecture.
    return popcount(a ^ b);
}

namespace detail {
using sketch::popcnt_fn;

inline unsigned unrolled_bitdiff(const uint64_t *a, const uint64_t *b, size_t nbytes);
inline unsigned byte_bitdiff(const uint8_t *a, const uint8_t *b, size_t nelem) {
    using Space = vec::SIMDTypes<uint64_t>;
    using SIMDHolder = typename vec::SIMDTypes<::std::uint64_t>::VType;
    size_t nblocks(nelem / (sizeof(SIMDHolder)));
    const SIMDHolder *pa((const SIMDHolder *)a), *pb((const SIMDHolder *)b);
    SIMDHolder tmp;
    tmp = (pa++)->simd_ ^ (pb++)->simd_; // I'm being lazy here and assuming it's aligned, but I have that guarantee from the aligned vectors.
    auto sum = popcnt_fn(tmp);
    while(--nblocks) { // Prefix decrement to account for the fact that I used one block in initialization.
        tmp.simd_ = (pa++)->simd_ ^ (pb++)->simd_;
        Space::add(sum, popcnt_fn(tmp));
    }
    unsigned ret = unrolled_bitdiff((const uint64_t *)pa, (const uint64_t *)pb, nelem & ((nelem / (sizeof(SIMDHolder) / sizeof(uint8_t))) - 1));
    for(size_t i = 0; i < sizeof(sum) / sizeof(uint64_t); ++i)
        ret += ((uint64_t *)&sum)[i];
    return ret;
}

inline unsigned unrolled_bitdiff(const uint64_t *a, const uint64_t *b, size_t nbytes) {
    unsigned ret(popcount(*a++ ^ *b++));
    nbytes -= 8;
    const size_t len(nbytes >> 3); // Num 64-bit integers.
#define ITER ret += popcount(*a++ ^ *b++)
    DO_DUFF(len, ITER);
#undef ITER
    if(__builtin_expect(nbytes &= 0x7u, 0)) {
        // I still haven't finished testing if this is correct or makes a big/little-endian mistake.
        // But, at worst, it's a matter of switching the right-shift for a left-shift.
        // ret += popcount((*a ^ *b) & ((0xFFFFFFFFFFFFFFFF >> ((8u - nbytes) << 3))));
        // The following has one less instruction in the assembly by using
        // "lea ecx, [64+rdx*8]", which puts the multiplication into the lea.
        ret += popcount((*a ^ *b) & ((0xFFFFFFFFFFFFFFFF >> ((8u - nbytes) * 8))));
    }
    return ret;
}
} // namespace detail

template<typename T>
inline auto vec_bitdiff(const T &a, const T &b) {
#if defined(USE_UNROLLED_BITDIFF)
    return detail::unrolled_bitdiff((const uint64_t *)&a[0], (const uint64_t *)&b[0], a.size() * sizeof(a[0]));
#else
    return detail::byte_bitdiff(&a[0], &b[0], a.size() * sizeof(a[0]));
#endif
}

} // namespace popcnt
